{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **AAI EXPERIMENT NO - 4**"
      ],
      "metadata": {
        "id": "7VHu83A3IiTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Aim - Build and Train a Generative Multi-Layer Network Model**"
      ],
      "metadata": {
        "id": "OdfY8atfIXfw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zZrSsF6AIKNu",
        "outputId": "adac40fe-1a02-4f9d-f44b-fd06ad857099"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 46ms/step - d_loss: 0.2158 - g_loss: 3.3893\n",
            "Epoch 2/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 48ms/step - d_loss: 0.0988 - g_loss: 3.8214\n",
            "Epoch 3/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - d_loss: 0.0732 - g_loss: 5.3056\n",
            "Epoch 4/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 51ms/step - d_loss: 0.0553 - g_loss: 5.9333\n",
            "Epoch 5/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 46ms/step - d_loss: 0.0529 - g_loss: 6.1102\n",
            "Epoch 6/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 49ms/step - d_loss: 0.0676 - g_loss: 6.3960\n",
            "Epoch 7/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 49ms/step - d_loss: 0.0358 - g_loss: 5.9425\n",
            "Epoch 8/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 46ms/step - d_loss: 0.0628 - g_loss: 7.0129\n",
            "Epoch 9/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 49ms/step - d_loss: 0.0549 - g_loss: 7.1683\n",
            "Epoch 10/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - d_loss: 0.0454 - g_loss: 7.4044\n",
            "Epoch 11/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 53ms/step - d_loss: 0.0403 - g_loss: 7.3569\n",
            "Epoch 12/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 46ms/step - d_loss: 0.0298 - g_loss: 6.2133\n",
            "Epoch 13/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 49ms/step - d_loss: 0.0369 - g_loss: 8.3683\n",
            "Epoch 14/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 47ms/step - d_loss: 0.0219 - g_loss: 6.3291\n",
            "Epoch 15/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 47ms/step - d_loss: 0.0855 - g_loss: 10.9241\n",
            "Epoch 16/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - d_loss: 0.0421 - g_loss: 8.3187\n",
            "Epoch 17/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 52ms/step - d_loss: 0.0478 - g_loss: 7.6353\n",
            "Epoch 18/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 50ms/step - d_loss: 0.0297 - g_loss: 6.9507\n",
            "Epoch 19/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - d_loss: 0.0320 - g_loss: 9.2808\n",
            "Epoch 20/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - d_loss: 0.0389 - g_loss: 7.1746\n",
            "Epoch 21/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - d_loss: 0.0166 - g_loss: 6.5708\n",
            "Epoch 22/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 46ms/step - d_loss: 0.0015 - g_loss: 8.4220\n",
            "Epoch 23/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 49ms/step - d_loss: 1.6419e-04 - g_loss: 10.4829\n",
            "Epoch 24/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 49ms/step - d_loss: 2.0511e-05 - g_loss: 11.4232\n",
            "Epoch 25/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 47ms/step - d_loss: 3.9181e-05 - g_loss: 11.2746\n",
            "Epoch 26/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 49ms/step - d_loss: 1.4520e-04 - g_loss: 10.8891\n",
            "Epoch 27/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - d_loss: 0.1302 - g_loss: 14.3872\n",
            "Epoch 28/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 49ms/step - d_loss: 0.0253 - g_loss: 6.7225\n",
            "Epoch 29/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 58ms/step - d_loss: 0.0031 - g_loss: 7.2914\n",
            "Epoch 30/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - d_loss: 7.5830e-04 - g_loss: 8.3108\n",
            "Epoch 31/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 49ms/step - d_loss: 2.7588e-04 - g_loss: 9.1006\n",
            "Epoch 32/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - d_loss: 5.6778e-04 - g_loss: 9.7155\n",
            "Epoch 33/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 49ms/step - d_loss: 4.6886e-05 - g_loss: 11.2279\n",
            "Epoch 34/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 49ms/step - d_loss: 1.2441e-05 - g_loss: 12.3085\n",
            "Epoch 35/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 46ms/step - d_loss: 1.0742e-05 - g_loss: 12.3945\n",
            "Epoch 36/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - d_loss: 7.7032e-06 - g_loss: 12.9082\n",
            "Epoch 37/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - d_loss: 1.4275e-05 - g_loss: 12.4343\n",
            "Epoch 38/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 49ms/step - d_loss: 0.0480 - g_loss: 15.9960\n",
            "Epoch 39/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - d_loss: 0.0180 - g_loss: 7.7097\n",
            "Epoch 40/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 49ms/step - d_loss: 0.0130 - g_loss: 9.8283\n",
            "Epoch 41/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - d_loss: 1.6026e-04 - g_loss: 11.4976\n",
            "Epoch 42/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 47ms/step - d_loss: 0.0156 - g_loss: 11.8384\n",
            "Epoch 43/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - d_loss: 4.1913e-04 - g_loss: 12.9201\n",
            "Epoch 44/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 48ms/step - d_loss: 3.6903e-05 - g_loss: 11.4717\n",
            "Epoch 45/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 49ms/step - d_loss: 0.0136 - g_loss: 12.5632\n",
            "Epoch 46/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 49ms/step - d_loss: 6.0875e-05 - g_loss: 10.9194\n",
            "Epoch 47/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 47ms/step - d_loss: 4.8644e-05 - g_loss: 11.2493\n",
            "Epoch 48/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 50ms/step - d_loss: 8.0829e-06 - g_loss: 12.9198\n",
            "Epoch 49/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 49ms/step - d_loss: 3.9415e-06 - g_loss: 13.9110\n",
            "Epoch 50/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 47ms/step - d_loss: 0.0015 - g_loss: 12.8428\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x200 with 10 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxoAAABVCAYAAADOppJ2AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAFYNJREFUeJzt3V1wVAcZxvFn9+xuNkuaZLPZXUK6AWKiNJkhpEUEDNYmWKtWaSu2M2i1XKkdmdGCN17VGfUCHcdRZyzgdJwWZ7SMFKEVsTe1Wik1pEgLJA0fBjLLko8Nu0nY7/N6gaxdw8fystldwvObORfNOXty9j+HZt6cnLMWEREQEREREREVkLXUB0BERERERHMPBw0iIiIiIio4DhpERERERFRwHDSIiIiIiKjgOGgQEREREVHBcdAgIiIiIqKC46BBREREREQFx0GDiIiIiIgKzpbvhhaLZTaP47ah+XxDtruM7fRuth27XcZzTo/t9NhOj+302E6PP2N18unGKxpERERERFRwHDSIiIiIiKjgOGgQEREREVHBcdAgIiIiIqKC46BBREREREQFx0GDiIiIiIgKjoMGEREREREVXN6fozGbDMOAzfa/QxERpNNpmKap3qfVas3uM5VKqZ4vfTtgOz2202E3PbbTYzs9ttNjOz2205lr3cpi0FizZg0+85nPZCNcunQJL7/8Mvr6+tT7bG9vxxe/+EWkUim89NJLGBwcLNThlhW202M7HXbTYzs9ttNjOz2202M7nTnXTfIEYFYWi8UiW7ZskVgsJplMRjKZjIyNjcmGDRtuab+PPvqoBINBOXXqlPT09BTseDXYju2K3Y7deM6xHduxHdux3ey0Y7f8u5X8ioaIYHBwEHv37oXf78e9996LiooKfPSjH0UsFpuxfTqdxrvvvot///vfCAQC6OjoQDweR19fH8LhcHa7YDCI/fv3I5lMYmRkpJhvqWjYTo/tdNhNj+302E6P7fTYTo/tdOZkt1JPbwDE5XKJ1+uVhx9+WAYHByWTycjFixclFArNWE6dOiVPPvmkAJAvfelLMjAwIH/7299k+fLlOfusqKiQ+vp68Xg8YrfbSzb1sh3blaIdu/GcYzu2Yzu2Y7vZacdu+Xcr+hUNp9OJ+vp6GIYxY11FRQWsViusVitqampQU1OTXZdIJDA+Po54PI6amhosWrQId999N/x+PzKZDOx2OwDA7Xajuroaly5dwvj4+C3dPFNu2E6P7XTYTY/t9NhOj+302E6P7XTuhG5FHzTa29vxzDPPwOfzzVhXW1sLv99/1dedPXsWP/nJT3Du3Dl0d3djx44daGhogMvlym5jt9vxxBNP4LHHHsObb76Jn//855iYmJi191JsbKfHdjrspsd2emynx3Z6bKfHdjp3QreiDxoejwddXV0IBAIwTRMiAovFAqvVCovFMmN70zRhmiYuXryIQ4cO4dSpU1i3bh16enpmbG+1WtHa2oqenh5MTk7C4XAU620VBdvpsZ0Ou+mxnR7b6bGdHtvpsZ3OndCtZDeDT01NYd++fRgYGMDq1avR09OT89xgABARHD58GAcOHMDZs2cRCoVKdLTlhe302E6H3fTYTo/t9NhOj+302E5nLncr2aAxPT2NXbt24ZVXXsEzzzyDT37ykzOiAkBfXx+2bt2K6elpmKaJqqqqEhxteWE7PbbTYTc9ttNjOz2202M7PbbTmcvdZmXQ8Pl8uOeee2CaJo4dO5bziK0rHA4HOjo6EIvF0NLSAqvViomJCRw7dgzT09PZ7SKRCD7xiU9gfHwcx44dQyqVwvHjx/GXv/wlu004HMZHPvIR1NXVYeHChVe93HS7YDs9ttNhNz2202M7PbbTYzs9ttO547vNxqO8HnroIent7ZW///3v8vGPfzxn3YMPPihDQ0OSTqdlZGREhoaGJBwOi2ma8vbbb8v9998vgUBAAoGANDU1yaZNm6S/v19effVVaWtrE4vFIm63O7tNIBCQNWvWyCuvvCJDQ0MSiUTENE35wx/+IH6/v+CPHdNgO7Yrdjt24znHdmzHdmzHdrPTjt3y7zYrVzQsFgvsdjsymQysVmvOukQigZGRETidTlRXV6O+vh7T09O4cOECxsbGEI/HkUgkEI1GEY/HEYlEYLPZYLfbs1NZJpNBKpXK2a/P50MgEMDk5CRCoRAmJiZuy8efsZ0e2+mwmx7b6bGdHtvpsZ0e2+nc6d0s/53MbrzhTVx6aWhowNKlS5HJZHDkyBGMjY1l13m9XixbtgwLFizAN7/5TaxYsQJ79uzBb3/7W9TV1aGrqwuGYWD79u144403sGjRIrS3t2NqagrvvPMOkskkNm7ciE9/+tPZfVZVVaGzsxNVVVXYuXMn9u7di2AwiKNHjyKRSNxEjhvLM1cOtruM7fRuth27XcZzTo/t9NhOj+302E6PP2N18uo2G5eJ8lnmz58ve/bskUwmI1u3bhWHwyGrV6+WEydOyNjYmGzYsEEsFkvOAkCqqqrkV7/6lWQyGclkMmKaZvYYY7GYfOc738luOxuLBtuxXbHbsRvPObZjO7ZjO7abnXbsln+3kj116mqCwSCef/55eDweNDc3Y/Pmzdl1IyMj2L9/PyKRCF577TVMTk6ivb0d3d3dcDqdJTzq8sB2emynw256bKfHdnpsp8d2emynM2e6ldP0ZrFYxG63i8/nkxdeeEHi8Xh2eeutt2Tp0qUCQAzDEIfDIRs3bpRwOFx20xvbsV2p27Ebzzm2Yzu2Yzu2m5127JZ/t6Jf0aitrUVLSwsaGhpQV1eXs66qqgqtra3w+Xzw+/2oqKjIrnO73ejs7MRdd92V/Vpzc/NVnzM8V7GdHtvpsJse2+mxnR7b6bGdHtvp3Andin5E7e3t+MEPfoBAIAC/35+zrrW1FT/84Q/R0tICn8+Xs66pqQnPPvtszo0s1dXVcLlcRTnucsB2emynw256bKfHdnpsp8d2emyncyd0K9qgUVlZCafTCY/Hg/r6etTW1iKRSCAej8M0Tbjdbvj9frS0tKClpWXmgdpscLvdyGQycLlc5fc3aLOI7fTYTofd9NhOj+302E6P7fTYTudO6laUQcNqteKzn/0s1q1bh3A4jJ/97GeIxWLZ9c3Nzfjxj38Mv98/Y2q7Ynh4GNu3b8eFCxewYcMGdHd3l/7TDouA7fTYTofd9NhOj+302E6P7fTYTueO61aMG18Mw5Bnn31WUqmU/PnPf5ZAIJBdZ7FYZMuWLRKPx6/7/Q8fPizLli2Tqqoqee6553Ie4WWappimWTY3vrAd25W6HbvxnGM7tmM7tmO72WnHbvl3K8oVDRFBb28vtm3bhoGBAUxNTd30Purr67F+/XqMjo6ira0tZ93Fixfx+uuvY3h4GEePHlV9aE25Yjs9ttNhNz2202M7PbbTYzs9ttO547oVY3oDIHa7XVwul1RUVORMV/lOb+l0Wi5duiRTU1OSTCZz1vX390t3d7e4XC6x2WyzNrndRC62Y7uStmM3nnNsx3Zsx3ZsNzvt2C3/bkW7GTyVSiGVSl113YULF/DOO+/AbrcDuHyTSyAQyHnUVyKRwNDQEBKJBBobG1FfX5/9ezSHw4GmpiaEw2EEg0GMjIzM/hsqIrbTYzsddtNjOz2202M7PbbTYzudO6pbsaa36y1er1eWLl0qHR0d0tHRIWvWrJFXX3015/ufOHFCHn/8cVm1apXs2rVrxsesv//++3Lo0CH5yle+UvLpje3YrtTt2I3nHNuxHduxHdvNTjt2y79bWXyyx+joKEZHR2G1WuF0OlFXV4fJycmcbRKJBM6dO4fTp08jGo3mrHM6nWhtbUU8HofX64XFYin936QVCdvpsZ0Ou+mxnR7b6bGdHtvpsZ3OXOtWFoPGFYsXL8bXvvY1NDU14b777stZ19jYiM2bNyMajWLlypUlOsLyxXZ6bKfDbnpsp8d2emynx3Z6bKczV7qV1aAxf/58PP744/jwhz88Y53H48Fjjz2W/e+yfV5wibCdHtvpsJse2+mxnR7b6bGdHtvpzJVuZTFotLW1obOzE0uWLEF1dfVVg91sxEAggI997GOorKwEAIgIjh49infffTd7CcnpdGLlypW4++67cfz4cRw5cgSmad76GyoittNjOx1202M7PbbTYzs9ttNjO505163UN75YLBb51re+JaFQSKLRqKTT6XwPaYYPfjjJww8/LP39/RIOhyUcDsvIyIh897vfFcMwst/b5/PJzp07ZXx8XL7//e+Lw+Eo+A1DbMd2pWjHbjzn2I7t2I7t2I4/Y0vdrWRXNAzDQENDA+666y40NTXB7XbD4XAAADKZDEKh0IwbXIDLH93u8/lQW1ubnegSiQTOnz+PaDQKm82GJUuWYPHixfB4PHC73QCAdDoNp9OZsy/TNHHx4kWEQiFMTk7eNjcZsZ0e2+mwmx7b6bGdHtvpsZ0e2+nM5W4lGzRqamqwadMmrFmzBgsWLIDN9r9DmZ6exo4dO3DgwIEZr3O5XNi0aRPWrVuX/dr58+fxox/9CCdOnMDnPvc5/PrXv4bH40F1dfV1jyESieAXv/gFdu7ciWAwiHQ6Xbg3OIvYTo/tdNhNj+302E6P7fTYTo/tdOZyt5INGna7Hc3Nzejs7AQAJJNJWK1W2Gw2pNNpDA4O4q233oJhGLDZbBARpFIpuFwuPProo0gkEtl18Xgc77//Po4ePYr169dj5cqVsFqtAJB9XTKZRCaTyTmGVCqFgYGBor/3W8V2emynw256bKfHdnpsp8d2emynM5e7lWzQmJycxIsvvoh//OMf2a8tXrwYTzzxRM4kt2LFCnzhC1/A6Ogofve732F0dBR79uzB6dOnsXz5cjzyyCPwer34xje+gQsXLqCrqyvnJpmJiQn8/ve/x8DAAA4dOnRb3RB0LWynx3Y67KbHdnpsp8d2emynx3Y6c7pbvjeVYBZvfLmydHV1SX9/v4yPj8uGDRsEgHz961+XaDQqvb290tHRkfO6r371qxIOh8U0TTFNUzKZTM6nI4qInDlzRtauXSsWi6Ugx6vBdmxX7HbsxnOO7diO7diO7WanHbvl361kVzQqKirQ2dmJhoaG7Ne8Xi8OHjwIq9WKc+fOAQDOnDmDffv24fz584hEItltRSR7o8qVae1aj/v64LbX0traira2NkxMTKCvrw9TU1O39P5mE9vpsZ0Ou+mxnR7b6bGdHtvpsZ3OnO5WqunN5/PJb37zGwmFQtll//79smLFCqmvr5eKigoBIJWVleL1eqWurk5sNlvOPp588kkJh8PXPe4zZ85IT0/PDSfIp59+Ws6ePSt79+6VD33oQwWbetmO7UrRjt14zrEd27Ed27Edf8aWulvJrmiICJLJJGKxGKqrq+F2u1FfX4958+ahsrISsVgMiUQCsVgMsVgs+zqLxYK6ujpUVVWhvr4+e4PL/5uensb4+DiGh4dzXn8tqVQq+z3L/W/92E6P7XTYTY/t9NhOj+302E6P7XTmdLdSTW8Oh0OWLl0qa9eulW3btkkymZSJiQl58803Zffu3dLd3X3V11VWVsq3v/1tee211+S9996TZDJ51eN94403ZP369bJ69Wpxu903PJ6mpiZ54IEH5L777hOXy1XWvzFgO7ZjN55zbMd2bMd2bFeaduyWf7eS3gxutVrFbrfLli1bZGpqSlKplKRSKRkZGZEvf/nLYhiGWK3W7PaGYUhNTY1s27Yt5wYX0zQlnU5LKpXK3vyye/duWbBggRiGUbCbXsrlHzLbsR278ZxjO7ZjO7Zju9K0Y7f8u1n+G+yGrnVTiVZVVRU+//nPY8mSJTBNc8aNLFeW3t5eHDhwAB6PB4888ggWLlyIT33qU7j33nuzxzQ6Ooo//vGPCAaDWLt2LVatWoX+/n786U9/wvDwMPbu3YvTp08X5LjzzJWD7S5jO72bbcdul/Gc02M7PbbTYzs9ttPjz1idvLqVanrz+/2ye/duSSaTsnXrVqmsrBTDMMQwDPF4PPLiiy9KKpWSX/7yl+JyuWTZsmXS29sr6XRaMplMzrEdP35curq6xOVyyU9/+tPsBJdOp+XkyZPXvOSkWTTYju2K3Y7deM6xHduxHdux3ey0Y7f8u5XsZnCLxQKr1QrDMLBo0SI88MAD2Y87r6qqwoIFC2AYBhYuXIi1a9eisbERtbW1MAxjxr7mzZuH5cuXo6amBk1NTdnJzzAMGIZxzcnT4XCgra0NXq8XQ0NDGBwcVP1GoNjYTo/tdNhNj+302E6P7fTYTo/tdOZ0t1JNb/Pnz5c9e/aIaZoSiURkaGgou5w7d06mpqbENE2JRqNy9uxZCQaDkkgkrnpsyWRSzp8/L0NDQxKJRHL+Vu16j/Lyer2yY8cOOXPmjHzve98Tu91+W/zGgO3Yjt14zrEd27Ed27FdadqxW/7dSnZFI5PJYGJiAqFQCABgt9vhcDhQU1MDq9WKaDSKUCgEl8uFxsbGaz6y68pr58+fn/O1eDyOaDSK8fFxzJs3D36/H1NTU5iens7ZzmazweFwXHUqLFdsp8d2Ouymx3Z6bKfHdnpsp8d2OnO5W8luBq+oqEBHR0dOjHvuuQebNm1CTU0Ntm/fjr/+9a946KGHsHHjRjidzpva/8GDB/Hcc88hnU6jq6sLfr8fL730Enbt2pV9JvAHj+HkyZM4ceLEDS8T5ZkrB9tdxnbFa8dul/GcY7vrYTu2uxq207tT2rHbZfl0K9kVjUQigbfffjvna2NjY3jqqafgdDpx+PBh7Nu3D42NjchkMtltrrypG725YDCIAwcOwOl04qmnnsL999+PI0eO5JwcVzuG2wHb6bGdDrvpsZ0e2+mxnR7b6bGdzlzuVrJB42qCwSCef/55eDweNDc3Y/PmzVi1ahXsdnt2m1Qqhddffx3/+te/rruv9957D9PT00gmk9i1axf6+vpw8ODBsv5kyFvBdnpsp8Nuemynx3Z6bKfHdnpspzNnuuV1J4fMzoeT/P9isVjEbreLz+eTF154QeLxuKRSqZwbWSYnJ+Xpp58Wh8Nx3cVms2X3a7PZxOFwiGEYt3yMGmzHdsVux24859iO7diO7dhudtqxW/7dyuqKhogglUohFovh5MmT+Oc//zljm1gshuHhYSSTybz3e+URYXMZ2+mxnQ676bGdHtvpsZ0e2+mxnc5c6Vaym8Gvx2q1wufzobq6esY60zQxOjqKSCRStOP5oDxz5WC7y9hO72bbsdtlPOf02E6P7fTYTo/t9PgzViefbmU5aJSzcv+HXM7YTq+c/ydYznjO6bGdHtvpsZ0e2+nxZ6xOPt2u/SBeIiIiIiIiJQ4aRERERERUcBw0iIiIiIio4DhoEBERERFRwXHQICIiIiKigsv7qVNERERERET54hUNIiIiIiIqOA4aRERERERUcBw0iIiIiIio4DhoEBERERFRwXHQICIiIiKiguOgQUREREREBcdBg4iIiIiICo6DBhERERERFRwHDSIiIiIiKrj/AG0co1kcbs4uAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "latent_dim = 100\n",
        "img_shape = (28, 28, 1)\n",
        "\n",
        "def build_generator():\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(128, activation='relu', input_shape=(latent_dim,)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dense(np.prod(img_shape), activation='sigmoid'),\n",
        "        layers.Reshape(img_shape)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "generator = build_generator()\n",
        "\n",
        "def build_discriminator():\n",
        "    model = keras.Sequential([\n",
        "        layers.Flatten(input_shape=img_shape),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "\n",
        "discriminator.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5),\n",
        "    loss=keras.losses.BinaryCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "class GAN(keras.Model):\n",
        "    def __init__(self, generator, discriminator):\n",
        "        super(GAN, self).__init__()\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "\n",
        "    def compile(self, g_optimizer, d_optimizer, loss_fn):\n",
        "        super(GAN, self).compile()\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        real_labels = tf.ones((batch_size, 1))\n",
        "        fake_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            real_loss = self.loss_fn(real_labels, self.discriminator(real_images))\n",
        "            fake_loss = self.loss_fn(fake_labels, self.discriminator(generated_images))\n",
        "            d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "        d_grads = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
        "        self.d_optimizer.apply_gradients(zip(d_grads, self.discriminator.trainable_variables))\n",
        "\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))\n",
        "        misleading_labels = tf.ones((batch_size, 1))\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            g_loss = self.loss_fn(misleading_labels, self.discriminator(self.generator(random_latent_vectors)))\n",
        "\n",
        "        g_grads = tape.gradient(g_loss, self.generator.trainable_variables)\n",
        "        self.g_optimizer.apply_gradients(zip(g_grads, self.generator.trainable_variables))\n",
        "\n",
        "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
        "\n",
        "gan = GAN(generator, discriminator)\n",
        "gan.compile(\n",
        "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5),\n",
        "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy()\n",
        ")\n",
        "\n",
        "(x_train, _), (_, _) = keras.datasets.mnist.load_data()\n",
        "x_train = (x_train.astype(\"float32\") / 255.0).reshape(-1, 28, 28, 1)\n",
        "\n",
        "gan.fit(x_train, epochs=50, batch_size=128)\n",
        "\n",
        "random_latent_vectors = np.random.normal(size=(10, latent_dim))\n",
        "generated_images = generator.predict(random_latent_vectors)\n",
        "\n",
        "fig, axes = plt.subplots(1, 10, figsize=(10, 2))\n",
        "for i, ax in enumerate(axes):\n",
        "    ax.imshow(generated_images[i].squeeze(), cmap='gray')\n",
        "    ax.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "e0o0VZQiJ3fI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}